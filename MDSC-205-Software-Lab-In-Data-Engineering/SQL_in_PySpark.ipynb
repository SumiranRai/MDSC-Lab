{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB5aAql+DPsBuh5I2OBYWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumiranRai/MDSC-Lab/blob/main/MDSC-205-Software-Lab-In-Data-Engineering/SQL_in_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sumiran Rai\n",
        "\n",
        "Regd. No. - 24040208007\n",
        "\n",
        "Software Lab In Data Engineering\n",
        "\n",
        "# SQL with PySpark"
      ],
      "metadata": {
        "id": "gYd-6F209Lee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing PySpark"
      ],
      "metadata": {
        "id": "qz3KDBsI-SP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T4br4IyujM8",
        "outputId": "a993f9f9-c5f9-4f56-ede5-033e421cf135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Spark Session and Loading the Datasets into Spark Dataframes"
      ],
      "metadata": {
        "id": "2baFwMEC-Ujh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"EmployeeData\").getOrCreate()\n",
        "\n",
        "# Read CSV file into a DataFrame with header and inferred schema\n",
        "employees= spark.read.csv(\"/content/employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Read CSV file into a DataFrame with header and inferred schema\n",
        "departments = spark.read.csv(\"/content/departments.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "fsdWEXQTu3Xz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the first few rows of the DataFrame for verification"
      ],
      "metadata": {
        "id": "tESeFwkV-nEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q__I0NKP-iXM",
        "outputId": "f7dba40c-1fe0-4253-bebe-b543f9f860fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+----------+------+----------+\n",
            "| id|  name|age|department|salary|experience|\n",
            "+---+------+---+----------+------+----------+\n",
            "|  1| Jack1| 48|        IT| 54831|        39|\n",
            "|  2|  Ivy2| 26|        HR| 99355|        24|\n",
            "|  3|David3| 22|        IT| 85694|        12|\n",
            "|  4|  Eva4| 59|   Support|113439|        24|\n",
            "|  5|Frank5| 31|        HR| 65103|        22|\n",
            "+---+------+---+----------+------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the first few rows of the DataFrame for verification"
      ],
      "metadata": {
        "id": "p0IkAmmE-r2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "departments.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OykSyXEp-g6R",
        "outputId": "ccf12bfa-72ba-45b4-b8f9-4c6e8bbaca22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|department|     location|\n",
            "+----------+-------------+\n",
            "|        HR|     New York|\n",
            "|        IT|San Francisco|\n",
            "|   Finance|       London|\n",
            "| Marketing|        Paris|\n",
            "|     Sales|       Berlin|\n",
            "+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the schema to verify column data types"
      ],
      "metadata": {
        "id": "q7WPQjxs9c6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4_qRHSZ-v2N",
        "outputId": "9b577141-bb06-435f-b044-5c03db444aac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- experience: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create or replace temporary views for the 'employees' and 'departments' DataFrames.\n",
        "\n",
        "These views allow SQL queries to be executed on the DataFrames within the current Spark session."
      ],
      "metadata": {
        "id": "gUP_2UO3Bf8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.createOrReplaceTempView(\"employees\")\n",
        "departments.createOrReplaceTempView(\"departments\")"
      ],
      "metadata": {
        "id": "k5WICeHx-wT5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Returns the rows where Salary > 60000 and Name contain 'D'"
      ],
      "metadata": {
        "id": "rCLxNNLn_Y8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"SparkSQLDemo\").getOrCreate()\n",
        "\n",
        "# Load CSV into DataFrame\n",
        "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Register DataFrame as SQL Table\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# Run SQL Query\n",
        "result = spark.sql(\"SELECT * FROM employees WHERE salary > 60000 and name like '%D%'\")\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExHCvkFp_Fxj",
        "outputId": "84040172-58a0-4829-9590-6bfad4a0c8e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---+----------+------+----------+\n",
            "| id|    name|age|department|salary|experience|\n",
            "+---+--------+---+----------+------+----------+\n",
            "|  3|  David3| 22|        IT| 85694|        12|\n",
            "| 22| David22| 23| Marketing| 60725|        37|\n",
            "| 27| David27| 59|   Finance|100104|        26|\n",
            "| 28| David28| 37|     Sales| 77333|        31|\n",
            "| 50| David50| 37| Marketing| 85070|        35|\n",
            "| 58| David58| 31|        HR| 71062|        12|\n",
            "| 69| David69| 57|     Sales| 94519|        14|\n",
            "| 74| David74| 56|        HR| 65828|         4|\n",
            "| 80| David80| 28|        IT| 98780|        23|\n",
            "| 91| David91| 44|        IT| 65134|        40|\n",
            "|123|David123| 27| Marketing| 61655|         1|\n",
            "|190|David190| 49| Marketing| 86555|        21|\n",
            "+---+--------+---+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing an Inner Join\n",
        "\n",
        "Definition - Returns only matching rows from both tables.\n",
        "If there is no match, the row is excluded.  \n",
        "\n",
        "Explanation:\n",
        "\n",
        "Retrieves employees who have a matching department in the departments table.\n",
        "If an employee's department is missing from departments, they are excluded."
      ],
      "metadata": {
        "id": "UYpvHF1u_mVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load departments dataset\n",
        "dept_df = spark.read.csv(\"departments.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Register DataFrame as SQL Table\n",
        "dept_df.createOrReplaceTempView(\"departments\")\n",
        "\n",
        "# Perform an INNER JOIN\n",
        "join_result = spark.sql(\"\"\"\n",
        "    SELECT e.name, e.department, d.location\n",
        "    FROM employees e\n",
        "    INNER JOIN departments d\n",
        "    ON e.department = d.department\n",
        "\"\"\")\n",
        "\n",
        "join_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iotpoi9I_nZB",
        "outputId": "b5726fe7-31df-42b6-a51a-b6c9db2674a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+-------------+\n",
            "|     name| department|     location|\n",
            "+---------+-----------+-------------+\n",
            "|    Jack1|         IT|San Francisco|\n",
            "|     Ivy2|         HR|     New York|\n",
            "|   David3|         IT|San Francisco|\n",
            "|     Eva4|    Support|      Chicago|\n",
            "|   Frank5|         HR|     New York|\n",
            "|   Alice6|  Marketing|        Paris|\n",
            "|   Alice7|  Marketing|        Paris|\n",
            "|   Alice8|    Support|      Chicago|\n",
            "|   Frank9|Engineering|      Seattle|\n",
            "|Charlie10|    Finance|       London|\n",
            "|Charlie11|         HR|     New York|\n",
            "|  Grace12|    Finance|       London|\n",
            "|    Ivy13|    Finance|       London|\n",
            "|  Grace14|         HR|     New York|\n",
            "|    Ivy15|         IT|San Francisco|\n",
            "|  Alice16|      Sales|       Berlin|\n",
            "|Charlie17|    Support|      Chicago|\n",
            "|  David18|      Sales|       Berlin|\n",
            "|    Eva19|         HR|     New York|\n",
            "|    Eva20|Engineering|      Seattle|\n",
            "+---------+-----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing a Left Join (Left Outer Join)\n",
        "\n",
        "Definition - Returns all rows from the left table (employees).\n",
        "If there is no match in the right table (departments), it returns NULL in those columns."
      ],
      "metadata": {
        "id": "nZaWw8CDDIBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a LEFT JOIN using PySpark\n",
        "join_result = spark.sql(\"\"\"\n",
        "    SELECT e.name, e.department, d.location\n",
        "    FROM employees e\n",
        "    LEFT JOIN departments d\n",
        "    ON e.department = d.department\n",
        "\"\"\")\n",
        "\n",
        "# Show results\n",
        "join_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBPv8NBJDGPA",
        "outputId": "f2100738-d1f7-4896-f22d-a0ff6ae5a7d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+-------------+\n",
            "|     name| department|     location|\n",
            "+---------+-----------+-------------+\n",
            "|    Jack1|         IT|San Francisco|\n",
            "|     Ivy2|         HR|     New York|\n",
            "|   David3|         IT|San Francisco|\n",
            "|     Eva4|    Support|      Chicago|\n",
            "|   Frank5|         HR|     New York|\n",
            "|   Alice6|  Marketing|        Paris|\n",
            "|   Alice7|  Marketing|        Paris|\n",
            "|   Alice8|    Support|      Chicago|\n",
            "|   Frank9|Engineering|      Seattle|\n",
            "|Charlie10|    Finance|       London|\n",
            "|Charlie11|         HR|     New York|\n",
            "|  Grace12|    Finance|       London|\n",
            "|    Ivy13|    Finance|       London|\n",
            "|  Grace14|         HR|     New York|\n",
            "|    Ivy15|         IT|San Francisco|\n",
            "|  Alice16|      Sales|       Berlin|\n",
            "|Charlie17|    Support|      Chicago|\n",
            "|  David18|      Sales|       Berlin|\n",
            "|    Eva19|         HR|     New York|\n",
            "|    Eva20|Engineering|      Seattle|\n",
            "+---------+-----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing a Right Join (Right Outer Join)\n",
        "\n",
        "Definition - Returns all rows from the right table (departments).\n",
        "If there is no match in the left table (employees), it returns NULL in those columns."
      ],
      "metadata": {
        "id": "dhTrFOmaDWa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a RIGHT JOIN using PySpark\n",
        "join_result = spark.sql(\"\"\"\n",
        "    SELECT e.name, e.department, d.location\n",
        "    FROM employees e\n",
        "    RIGHT JOIN departments d\n",
        "    ON e.department = d.department\n",
        "\"\"\")\n",
        "\n",
        "# Show results\n",
        "join_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gnyGoJVDXL4",
        "outputId": "56b546a7-75cb-4f31-8d39-4ebc0b1fb44d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+\n",
            "|      name|department|location|\n",
            "+----------+----------+--------+\n",
            "|   Jack199|        HR|New York|\n",
            "|Charlie194|        HR|New York|\n",
            "|   Jack192|        HR|New York|\n",
            "|    Bob167|        HR|New York|\n",
            "|  Alice155|        HR|New York|\n",
            "|    Ivy153|        HR|New York|\n",
            "| Hannah145|        HR|New York|\n",
            "|Charlie143|        HR|New York|\n",
            "|  Frank135|        HR|New York|\n",
            "|   Jack132|        HR|New York|\n",
            "|    Bob126|        HR|New York|\n",
            "|  David109|        HR|New York|\n",
            "| Hannah105|        HR|New York|\n",
            "|   Jack103|        HR|New York|\n",
            "| Hannah101|        HR|New York|\n",
            "|   Grace96|        HR|New York|\n",
            "|     Bob92|        HR|New York|\n",
            "|     Bob89|        HR|New York|\n",
            "|     Eva82|        HR|New York|\n",
            "|   David74|        HR|New York|\n",
            "+----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing a query on a Temporary View using Spark SQL"
      ],
      "metadata": {
        "id": "oMHmYtm3ER16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"TempViewDemo\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    Row(id=1, name=\"Alice\", age=25),\n",
        "    Row(id=2, name=\"Bob\", age=30),\n",
        "    Row(id=3, name=\"Charlie\", age=28)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Create a TEMPORARY VIEW\n",
        "df.createOrReplaceTempView(\"people_view\")\n",
        "\n",
        "# Query the temporary view using Spark SQL\n",
        "result = spark.sql(\"SELECT * FROM people_view WHERE age > 26\")\n",
        "\n",
        "# Show Results\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIjiBFg2_tYZ",
        "outputId": "6346c982-d0cb-494f-ce8a-f3bf24d7a646"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 28|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Temporary View in PySpark\n",
        "\n",
        "A Global Temporary View in PySpark is similar to a Temporary View, but it persists across multiple Spark sessions within the same application.\n",
        "\n",
        "\tQueried using global_temp.<view_name>\n",
        "  \n",
        "Exists as long as the application is running"
      ],
      "metadata": {
        "id": "Sjc9U3DvFC0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"GlobalTempViewDemo\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    Row(id=1, name=\"Alice\", age=25),\n",
        "    Row(id=2, name=\"Bob\", age=30),\n",
        "    Row(id=3, name=\"Charlie\", age=28)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Create a GLOBAL TEMPORARY VIEW\n",
        "df.createOrReplaceGlobalTempView(\"global_people_view\")\n",
        "\n",
        "# Query the global temporary view\n",
        "global_result = spark.sql(\"SELECT * FROM global_temp.global_people_view WHERE age < 30\")\n",
        "\n",
        "# Show Results\n",
        "global_result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udzwhBHS_wLh",
        "outputId": "140df18e-d116-4317-a599-bcaf6bf019a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 25|\n",
            "|  3|Charlie| 28|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quering the global temporary view with a new Spark Session\n",
        "\n",
        "Does not disappear when the session ends"
      ],
      "metadata": {
        "id": "ImW9rOvQFtO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a new Spark session\n",
        "spark2 = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
        "\n",
        "# Query the global temporary view\n",
        "new_result = spark2.sql(\"SELECT * FROM global_temp.global_people_view\")\n",
        "\n",
        "# Show Results\n",
        "new_result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khf5nnT5_xWB",
        "outputId": "049da969-5a85-4e07-b919-f0b235f5cf12"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 25|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 28|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use createOrReplaceTempView for quick in-session analysis."
      ],
      "metadata": {
        "id": "DbSHA4ipLNHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"SessionTempViewDemo\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [Row(id=1, name=\"Alice\", age=25), Row(id=2, name=\"Bob\", age=31)]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Create a SESSION TEMPORARY VIEW\n",
        "df.createOrReplaceTempView(\"people_view\")\n",
        "\n",
        "# Query the session temporary view\n",
        "session_result = spark.sql(\"SELECT * FROM people_view WHERE age > 26\")\n",
        "\n",
        "session_result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u8fMOt3_yKJ",
        "outputId": "6b3d00b9-0673-4378-8a16-ae9353f97592"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  2| Bob| 31|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use createOrReplaceGlobalTempView when you need to share the table across multiple Spark sessions."
      ],
      "metadata": {
        "id": "vRWyrPmZLWkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Stop the existing Spark session\n",
        "spark.stop()\n",
        "\n",
        "# Get an existing SparkSession or, if there is no existing one, create a new one\n",
        "new_spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
        "\n",
        "# Recreate the global temporary view in the new session\n",
        "# Sample Data\n",
        "data = [Row(id=1, name=\"Alice\", age=25), Row(id=2, name=\"Bob\", age=31)]\n",
        "\n",
        "# Create DataFrame\n",
        "df = new_spark.createDataFrame(data)  # Use new_spark to create the DataFrame\n",
        "\n",
        "# Create a SESSION TEMPORARY VIEW\n",
        "df.createOrReplaceGlobalTempView(\"people_view\") # Recreate the view\n",
        "\n",
        "# Query the session temporary view\n",
        "session_result = new_spark.sql(\"SELECT * FROM global_temp.people_view WHERE age > 26\")\n",
        "\n",
        "session_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um34zzsR_0Ip",
        "outputId": "34c83a93-8981-4d11-ce90-2379f6d1a32a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  2| Bob| 31|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Temporary View"
      ],
      "metadata": {
        "id": "jSSj6BIkMcT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Start a completely new session\n",
        "spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [Row(id=1, name=\"Alice\", age=25), Row(id=2, name=\"Bob\", age=31)]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data)  # Use new_spark to create the DataFrame\n",
        "\n",
        "# Create a GLOBAL TEMPORARY VIEW\n",
        "df.createOrReplaceGlobalTempView(\"global_people_view\")\n",
        "\n",
        "# Query the global temporary view\n",
        "# Use spark to query the view since it was created in the new session\n",
        "global_result = spark.sql(\"SELECT * FROM global_temp.global_people_view WHERE age < 30\")\n",
        "\n",
        "global_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvPrQvbL_26L",
        "outputId": "5cff73b4-e087-49cf-bb49-da37a9b9215f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+\n",
            "| id| name|age|\n",
            "+---+-----+---+\n",
            "|  1|Alice| 25|\n",
            "+---+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a new Spark session\n",
        "# Create a Spark session for handling large-scale data processing\n",
        "spark_new = SparkSession.builder.appName(\"AfterRestart\").getOrCreate()\n",
        "\n",
        "# query the spark session\n",
        "global_result = spark_new.sql(\"SELECT * FROM global_temp.global_people_view\")\n",
        "# Display the first few rows of the DataFrame for verification\n",
        "global_result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ywo1QuD_4lJ",
        "outputId": "0438c785-72a2-4290-a148-6b1d6ac24446"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+\n",
            "| id| name|age|\n",
            "+---+-----+---+\n",
            "|  1|Alice| 25|\n",
            "|  2|  Bob| 31|\n",
            "+---+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save DataFrame as a persistent table\n",
        "\n",
        "Use saveAsTable if you want to store data permanently for future use."
      ],
      "metadata": {
        "id": "xNMEZbnyMVNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DataFrame as a persistent table\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"permanent_people_tables\")\n",
        "\n",
        "# Query the saved table\n",
        "permanent_result = spark.sql(\"SELECT * FROM permanent_people_tables WHERE age < 30\")\n",
        "\n",
        "permanent_result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Vvs8sb_6y5",
        "outputId": "3564016e-e293-4887-b3d4-24fef1bc464d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+\n",
            "| id| name|age|\n",
            "+---+-----+---+\n",
            "|  1|Alice| 25|\n",
            "+---+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a new Spark session\n",
        "spark_new = SparkSession.builder.appName(\"AfterRestart\").getOrCreate()\n",
        "\n",
        "#  This will work because the table is persisted\n",
        "permanent_result = spark_new.sql(\"SELECT * FROM permanent_people_tables\")\n",
        "# Display the first few rows of the DataFrame for verification\n",
        "permanent_result.show()\n"
      ],
      "metadata": {
        "id": "VLJfHu6l_8OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d53ef3-f4b6-49d0-d568-12d4d62aeeee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+\n",
            "| id| name|age|\n",
            "+---+-----+---+\n",
            "|  1|Alice| 25|\n",
            "|  2|  Bob| 31|\n",
            "+---+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a session temporay view, global temporary view and a persistent table"
      ],
      "metadata": {
        "id": "ugAhgXceiGT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"TestViews\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    Row(id=1, name=\"Alice\", age=25),\n",
        "    Row(id=2, name=\"Bob\", age=30),\n",
        "    Row(id=3, name=\"Charlie\", age=28)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Create a session temporary view\n",
        "df.createOrReplaceTempView(\"session_people_view\")\n",
        "\n",
        "# Create a global temporary view\n",
        "df.createOrReplaceGlobalTempView(\"global_people_view\")\n",
        "\n",
        "# Create a persistent table\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"persistent_people_tables\")\n"
      ],
      "metadata": {
        "id": "2_vPgbdZ_9bT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stop the session"
      ],
      "metadata": {
        "id": "_HuvSDE2htgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "E0i1Jbwp_-DS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Start a new session"
      ],
      "metadata": {
        "id": "I5uDPdAjiOqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a new session\n",
        "new_spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
        "\n",
        "# Try accessing the session temporary view (Should FAIL)\n",
        "try:\n",
        "    new_spark.sql(\"SELECT * FROM session_people_view\").show()\n",
        "except Exception as e:\n",
        "    print(\"Session Temporary View: FAILED as expected!\", e)\n",
        "\n",
        "# Try accessing the global temporary view (Should WORK)\n",
        "try:\n",
        "    new_spark.sql(\"SELECT * FROM global_temp.global_people_view\").show()\n",
        "except Exception as e:\n",
        "    print(\"Global Temporary View: FAILED! This should have worked.\", e)\n",
        "\n",
        "# Try accessing the persistent table (Should WORK)\n",
        "try:\n",
        "    new_spark.sql(\"SELECT * FROM persistent_people_tables\").show()\n",
        "except Exception as e:\n",
        "    print(\"Persistent Table: FAILED! This should have worked.\", e)\n"
      ],
      "metadata": {
        "id": "ma4VkaBQ_-5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625532dc-07ac-4c0d-b83f-95d89db5db42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session Temporary View: FAILED as expected! [TABLE_OR_VIEW_NOT_FOUND] The table or view `session_people_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
            "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
            "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n",
            "'Project [*]\n",
            "+- 'UnresolvedRelation [session_people_view], [], false\n",
            "\n",
            "Global Temporary View: FAILED! This should have worked. [TABLE_OR_VIEW_NOT_FOUND] The table or view `global_temp`.`global_people_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
            "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
            "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n",
            "'Project [*]\n",
            "+- 'UnresolvedRelation [global_temp, global_people_view], [], false\n",
            "\n",
            "Persistent Table: FAILED! This should have worked. [TABLE_OR_VIEW_NOT_FOUND] The table or view `persistent_people_tables` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
            "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
            "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n",
            "'Project [*]\n",
            "+- 'UnresolvedRelation [persistent_people_tables], [], false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"EmployeeData\").getOrCreate()"
      ],
      "metadata": {
        "id": "ipBNLfvoABv5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", [\"Python\", \"SQL\"]),\n",
        "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
        "    (\"Charlie\", [\"C++\", \"R\"])\n",
        "]\n",
        "columns = [\"name\", \"skills\"]\n",
        "\n",
        "# Create DataFrame\n",
        "skills_df = spark.createDataFrame(data, columns)\n"
      ],
      "metadata": {
        "id": "yIN7Yg4iACLi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df.show()"
      ],
      "metadata": {
        "id": "yYpahtcHADFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb3c834-903e-4b02-b269-61eea85cc051"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+\n",
            "|   name|       skills|\n",
            "+-------+-------------+\n",
            "|  Alice|[Python, SQL]|\n",
            "|    Bob|[Java, Scala]|\n",
            "|Charlie|     [C++, R]|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df = skills_df.withColumn(\n",
        "    \"uppercase_skills\",\n",
        "    expr(\"transform(skills, x -> upper(x))\")\n",
        ")\n",
        "\n",
        "skills_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "dU1xBxWwAEXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7c1012-15e1-4658-b951-8fd47c3a8d8a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+----------------+\n",
            "|name   |skills       |uppercase_skills|\n",
            "+-------+-------------+----------------+\n",
            "|Alice  |[Python, SQL]|[PYTHON, SQL]   |\n",
            "|Bob    |[Java, Scala]|[JAVA, SCALA]   |\n",
            "|Charlie|[C++, R]     |[C++, R]        |\n",
            "+-------+-------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PySpark modules\n",
        "from pyspark.sql.functions import expr\n",
        "data = [\n",
        "    (\"Alice\", [\"Python\", \"SQL\"]),\n",
        "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
        "    (\"Charlie\", [\"C++\", \"R\"])\n",
        "]\n",
        "columns = [\"name\", \"skills\"]\n",
        "\n",
        "# Create DataFrame\n",
        "skills_df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "DM1fmJsqAFUR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df = skills_df.withColumn(\n",
        "    \"prefixed_skills\",\n",
        "    expr(\"transform(skills, x -> concat('Skill: ', x))\")\n",
        ")\n",
        "\n",
        "skills_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "uRdWBMqMAGfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a68c95-c78a-4169-b031-38f589c55abd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+---------------------------+\n",
            "|name   |skills       |prefixed_skills            |\n",
            "+-------+-------------+---------------------------+\n",
            "|Alice  |[Python, SQL]|[Skill: Python, Skill: SQL]|\n",
            "|Bob    |[Java, Scala]|[Skill: Java, Skill: Scala]|\n",
            "|Charlie|[C++, R]     |[Skill: C++, Skill: R]     |\n",
            "+-------+-------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df = skills_df.withColumn(\n",
        "    \"skill_lengths\",\n",
        "    expr(\"transform(skills, x -> length(x))\")\n",
        ")\n",
        "\n",
        "# Display the first few rows of the DataFrame for verification\n",
        "skills_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "OZMsNggPAHrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8425a0af-5819-4823-e14f-90fba5076f32"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+---------------------------+-------------+\n",
            "|name   |skills       |prefixed_skills            |skill_lengths|\n",
            "+-------+-------------+---------------------------+-------------+\n",
            "|Alice  |[Python, SQL]|[Skill: Python, Skill: SQL]|[6, 3]       |\n",
            "|Bob    |[Java, Scala]|[Skill: Java, Skill: Scala]|[4, 5]       |\n",
            "|Charlie|[C++, R]     |[Skill: C++, Skill: R]     |[3, 1]       |\n",
            "+-------+-------------+---------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df = skills_df.withColumn(\n",
        "    \"skill_lengths\",\n",
        "    expr(\"transform(skills, x -> CASE WHEN x = 'Python' THEN upper(x) ELSE x END)\")\n",
        ")\n",
        "\n",
        "skills_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "ym1yn7_XAJhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd45069-d9e5-4953-fec7-0de9ffdfbcf3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+---------------------------+-------------+\n",
            "|name   |skills       |prefixed_skills            |skill_lengths|\n",
            "+-------+-------------+---------------------------+-------------+\n",
            "|Alice  |[Python, SQL]|[Skill: Python, Skill: SQL]|[PYTHON, SQL]|\n",
            "|Bob    |[Java, Scala]|[Skill: Java, Skill: Scala]|[Java, Scala]|\n",
            "|Charlie|[C++, R]     |[Skill: C++, Skill: R]     |[C++, R]     |\n",
            "+-------+-------------+---------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Alice\", [\"Python\", \"SQL\"]),\n",
        "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
        "    (\"Charlie\", [\"C++\", \"R\"])\n",
        "]\n",
        "columns = [\"name\", \"skills\"]"
      ],
      "metadata": {
        "id": "SEO-3LJoAImZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df = skills_df.withColumn(\n",
        "    \"skill_lengths\",\n",
        "    expr(\"transform(skills, x -> CASE WHEN x = 'Python' THEN upper(x) ELSE x END)\")\n",
        ")\n",
        "\n",
        "skills_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "5Mb70SzzjG9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73373d8-06b4-43e5-dcd9-49019f0f8aaf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+---------------------------+-------------+\n",
            "|name   |skills       |prefixed_skills            |skill_lengths|\n",
            "+-------+-------------+---------------------------+-------------+\n",
            "|Alice  |[Python, SQL]|[Skill: Python, Skill: SQL]|[PYTHON, SQL]|\n",
            "|Bob    |[Java, Scala]|[Skill: Java, Skill: Scala]|[Java, Scala]|\n",
            "|Charlie|[C++, R]     |[Skill: C++, Skill: R]     |[C++, R]     |\n",
            "+-------+-------------+---------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}