{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcRef5sBIcv3cC6uqToai0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumiranRai/MDSC-Lab/blob/main/MDSC-205-Software-Lab-In-Data-Engineering/Streaming_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sumiran Rai\n",
        "\n",
        "Regd. No. - 24040208007\n",
        "\n",
        "Software Lab in Data Engineering\n",
        "\n",
        "# Operations on Streaming Data using PySpark"
      ],
      "metadata": {
        "id": "6G3jUFtaN6eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PySpark"
      ],
      "metadata": {
        "id": "MGJJL1v7OUM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-c5P3l3WNVOJ"
      },
      "outputs": [],
      "source": [
        "!pip -qq install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required packages"
      ],
      "metadata": {
        "id": "xWh8fYgEOYbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "6_Z3HjzjN5jg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting the Spark Session"
      ],
      "metadata": {
        "id": "9JT_FnztN5OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "vqQmlcwNOb4N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the Data"
      ],
      "metadata": {
        "id": "0EAkrCmlOk4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# header=True → Uses the firt row as column names.\n",
        "# inferSchema=True → Automatiscally detects data types.\n",
        "\n",
        "df=spark.read.csv('/content/TempHumi.csv',header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "tVGXjsz8OlYB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diwpjpRYO7sp",
        "outputId": "f0de2b14-342b-4c9b-f2c6-822e5a1261d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Temperatur (C)', 'Humidity (%)', 'Hour']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjKThGHlO8sZ",
        "outputId": "1e71dcce-3db8-43e9-8684-40fde4ace63e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+------------+----+\n",
            "|                Date|Temperatur (C)|Humidity (%)|Hour|\n",
            "+--------------------+--------------+------------+----+\n",
            "|12/12/2017 12:00:...|          16.1|      72.092|   0|\n",
            "|12/12/2017 12:05:...|          16.0|      72.092|   0|\n",
            "|12/12/2017 12:10:...|          15.9|      72.092|   0|\n",
            "|12/12/2017 12:15:...|         15.85|    72.04201|   0|\n",
            "|12/12/2017 12:20:...|         15.85|      72.342|   0|\n",
            "+--------------------+--------------+------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('Date').dtypes # Checking data type of any column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmm7wNXWO9wM",
        "outputId": "ad0be4e2-7cc8-4705-a761-f7589f876d8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Date', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Hour\").count().show(24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOH8jO6BPBOg",
        "outputId": "ba830456-c81c-4714-e180-2d0bbcf67228"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|Hour|count|\n",
            "+----+-----+\n",
            "|  12|   12|\n",
            "|  22|   12|\n",
            "|   1|   12|\n",
            "|  13|   12|\n",
            "|   6|   12|\n",
            "|  16|   12|\n",
            "|   3|   11|\n",
            "|  20|   12|\n",
            "|   5|   12|\n",
            "|  19|   12|\n",
            "|  15|   12|\n",
            "|   9|   12|\n",
            "|  17|   12|\n",
            "|   4|   13|\n",
            "|   8|   12|\n",
            "|  23|   12|\n",
            "|   7|   12|\n",
            "|  10|   12|\n",
            "|  21|   12|\n",
            "|  11|   12|\n",
            "|  14|   12|\n",
            "|   2|   12|\n",
            "|   0|   12|\n",
            "|  18|   12|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now save the output of that job by filtering on each step and saving it to a separate file"
      ],
      "metadata": {
        "id": "83ekHAYcPHkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir HourFolder  # Making a folder on current directory"
      ],
      "metadata": {
        "id": "QABxBpPCPFsk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = df.select(\"Hour\").distinct().collect()"
      ],
      "metadata": {
        "id": "kZ36EtpyPJ0W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(steps)\n",
        "print(type(steps))\n",
        "print(steps[0])\n",
        "print(steps[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb-68kcdPLRu",
        "outputId": "92bf86b3-09a7-44dd-e1e7-102d03ffec46"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(Hour=12), Row(Hour=22), Row(Hour=1), Row(Hour=13), Row(Hour=6), Row(Hour=16), Row(Hour=3), Row(Hour=20), Row(Hour=5), Row(Hour=19), Row(Hour=15), Row(Hour=9), Row(Hour=17), Row(Hour=4), Row(Hour=8), Row(Hour=23), Row(Hour=7), Row(Hour=10), Row(Hour=21), Row(Hour=11), Row(Hour=14), Row(Hour=2), Row(Hour=0), Row(Hour=18)]\n",
            "<class 'list'>\n",
            "Row(Hour=12)\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df.where(f\"Hour={steps[0][0]}\")\n",
        "df_test.show()\n",
        "type(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "B1ZhGN7RPM5M",
        "outputId": "5e815df1-78a1-4b09-c80c-67c755104bc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+------------+----+\n",
            "|                Date|Temperatur (C)|Humidity (%)|Hour|\n",
            "+--------------------+--------------+------------+----+\n",
            "|12/12/2017 12:00:...|     19.900002|   69.742004|  12|\n",
            "|12/12/2017 12:05:...|         19.95|      68.892|  12|\n",
            "|12/12/2017 12:10:...|         20.05|    69.04201|  12|\n",
            "|12/12/2017 12:15:...|         20.05|      68.942|  12|\n",
            "|12/12/2017 12:20:...|          20.3|      69.342|  12|\n",
            "|12/12/2017 12:25:...|         20.35|      69.092|  12|\n",
            "|12/12/2017 12:30:...|     20.400002|      68.842|  12|\n",
            "|12/12/2017 12:35:...|         20.45|      68.642|  12|\n",
            "|12/12/2017 12:40:...|          20.5|   68.742004|  12|\n",
            "|12/12/2017 12:45:...|         20.55|      68.592|  12|\n",
            "|12/12/2017 12:50:...|     20.599998|   68.242004|  12|\n",
            "|12/12/2017 12:55:...|     20.599998|    68.29201|  12|\n",
            "+--------------------+--------------+------------+----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we don't have a real-time data source, the notebook creates a simulated streaming environment by -\n",
        "\n",
        "Splitting Data into Hourly Chunks\n",
        "\n",
        "Each row has an \"Hour\" column. The dataset is split into multiple small CSV files, where each file contains data from a single hour."
      ],
      "metadata": {
        "id": "eX63lgt8PR9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Creating the streaming version of this input\n",
        "Implementing the above steps in the loop and making individual `csv` files for each hour step. These files are saved in a folder as given below. We will use this folder as a source of incoming stream data and, we will read each file one by one as if it is a stream."
      ],
      "metadata": {
        "id": "EwSsTiM8PfoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in steps:\n",
        "  df1 = df.where(f\"Hour={step[0]}\")\n",
        "  df1.coalesce(1).write.csv(path='/content/HourFolder/Hourly_Record',header=\"true\",mode=\"append\")"
      ],
      "metadata": {
        "id": "k3O1fZgZPXQt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Hourly_Record_list = [i for i in os.listdir(\"/content/HourFolder/Hourly_Record/\") if i.endswith(\".csv\")]\n",
        "Hourly_Record_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hlWtSGXPt7k",
        "outputId": "fab325ac-3e4c-4e48-d2cf-04cfba1bf784"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['part-00000-fbb7cfd8-9b6d-483a-a975-b17d736c2ae9-c000.csv',\n",
              " 'part-00000-29f0c0ae-a926-4b1e-9dc8-e93be30efa7c-c000.csv',\n",
              " 'part-00000-6702ebaf-4f8a-40b3-87b0-2423d9ae5050-c000.csv',\n",
              " 'part-00000-72bb344f-c03d-42ad-83b7-464ac959af64-c000.csv',\n",
              " 'part-00000-ca6fac55-59e3-4646-8a2a-bcefd7d73676-c000.csv',\n",
              " 'part-00000-aaf477e5-9b83-4247-bdca-9f5f334ff55c-c000.csv',\n",
              " 'part-00000-696224d1-4d86-4377-a055-a3bbcd332938-c000.csv',\n",
              " 'part-00000-e2c431c7-6224-46bf-981a-1a5eab430531-c000.csv',\n",
              " 'part-00000-c5176318-523a-4de4-958e-2e0c7ddf145b-c000.csv',\n",
              " 'part-00000-fff72b43-a3c5-40fc-b97c-424a03d5fc5a-c000.csv',\n",
              " 'part-00000-962b488d-e8f2-4bc2-bcca-4df147fc144e-c000.csv',\n",
              " 'part-00000-c07eb922-a833-4ab0-9984-498661f5a1bc-c000.csv',\n",
              " 'part-00000-f61fde5a-593b-421c-8acc-d688278ad473-c000.csv',\n",
              " 'part-00000-b5494da8-63a2-4927-b71a-ac26218d44b3-c000.csv',\n",
              " 'part-00000-078ce559-4020-4967-a7ac-4e66a9db781a-c000.csv',\n",
              " 'part-00000-94466a82-1dbb-47b3-864a-60adf2007211-c000.csv',\n",
              " 'part-00000-aaf3a808-5791-4843-96c6-1236cd5981df-c000.csv',\n",
              " 'part-00000-2dba52a9-f132-479d-96c3-f610f0509282-c000.csv',\n",
              " 'part-00000-e9b39a0b-b597-4d3b-980f-2442e36e3f5b-c000.csv',\n",
              " 'part-00000-4d69c39e-d269-4cde-97aa-971eaebd03c4-c000.csv',\n",
              " 'part-00000-aa9a4046-f583-461c-9946-28406bf8c9fe-c000.csv',\n",
              " 'part-00000-1bc4ae58-2a17-4d38-a893-c2a772ccd970-c000.csv',\n",
              " 'part-00000-fcef9530-d82b-4413-813e-41f6ba82d41e-c000.csv',\n",
              " 'part-00000-c606ed7f-809f-4060-b843-49ced58bc6e9-c000.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking any one file from above"
      ],
      "metadata": {
        "id": "q6ZMViiGfHq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = Hourly_Record_list[random.randint(0,len(Hourly_Record_list)-1)]\n",
        "file_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MqmNQ349PwNI",
        "outputId": "ce3d4a71-8942-41d8-8e61-37f3f58bbdb9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'part-00000-c07eb922-a833-4ab0-9984-498661f5a1bc-c000.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part =spark.read.csv(\"/content/HourFolder/Hourly_Record/\"+file_name, header=True,inferSchema=True)\n",
        "part.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUJ4drV8PzTd",
        "outputId": "7fcde850-61a5-49da-f0d5-387ce0ec1697"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+------------+----+\n",
            "|                Date|Temperatur (C)|Humidity (%)|Hour|\n",
            "+--------------------+--------------+------------+----+\n",
            "|12/12/2017 02:00:...|          15.5|   75.492004|   2|\n",
            "|12/12/2017 02:05:...|          15.5|      75.642|   2|\n",
            "|12/12/2017 02:10:...|     15.450001|    75.54201|   2|\n",
            "|12/12/2017 02:15:...|     15.450001|      75.842|   2|\n",
            "|12/12/2017 02:20:...|     15.450001|   75.992004|   2|\n",
            "|12/12/2017 02:25:...|          15.5|      75.942|   2|\n",
            "|12/12/2017 02:30:...|          15.5|      75.842|   2|\n",
            "|12/12/2017 02:35:...|          15.4|      75.942|   2|\n",
            "|12/12/2017 02:40:...|          15.4|      76.092|   2|\n",
            "|12/12/2017 02:45:...|          15.4|      76.342|   2|\n",
            "|12/12/2017 02:50:...|         15.45|      76.342|   2|\n",
            "|12/12/2017 02:55:...|     15.450001|    76.04201|   2|\n",
            "+--------------------+--------------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part.schema # checking the part schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swk6fGkZP3N1",
        "outputId": "7f8ce284-8376-416c-f415-cee836d2740d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('Date', StringType(), True), StructField('Temperatur (C)', DoubleType(), True), StructField('Humidity (%)', DoubleType(), True), StructField('Hour', IntegerType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streaming = (spark.readStream.schema(part.schema).option('maxFilesPerTrigger',1).csv('HourFolder/Hourly_Record/'))\n",
        "\n",
        "# maxFilesPerTrigger: maximum number of new files to be considered in every trigger, here taken 1.\n",
        "\n",
        "# File source - Reads files written in a directory as a stream of data, here directory: HourFolder/Hourly_Record.\n",
        "\n",
        "# Supported file formats are text, CSV, JSON, ORC, Parquet, here is CSV.\n",
        "\n",
        "# Files will be processed in the order of file modification time."
      ],
      "metadata": {
        "id": "bsldyHYcP4K0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(streaming)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "j9Y97kTfP5H1",
        "outputId": "ce20b82c-efb3-4d8c-a774-94c7fa35847a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Transformation\n",
        "\n",
        "* Hourly_Mean_Value is a streaming DataFrame (that is, a DataFrame on unbounded, streaming data) that represents the running mean that will be computed once the streaming query is started and the streaming input data is being continuously processed.\n",
        "\n",
        "* Now that we have our transformation, we need to specify an output sink for the results. for this example, we are going to write to a memory sink that keeps the results in memory.\n",
        "\n",
        "* We also need to define how spark will output that data. In this example, we will use the complete output mode (rewriting all of the keys along with their counts after every trigger).\n",
        "\n",
        "* In this example, we will not include activity query.awaitTermination() because it is required only to prevent the driver process from terminating when the stream is active. So to be able to run this locally in a notebook we will not include it."
      ],
      "metadata": {
        "id": "BOH-x11Gfa_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hourly_Mean_Value = streaming.groupBy('Hour').mean(\"Temperatur (C)\",\"Humidity (%)\").orderBy(F.desc(\"Hour\"))"
      ],
      "metadata": {
        "id": "60qbXtEWP6Q8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define output sink and output mode\n",
        "\n",
        "* Memory sink (for debugging) - The output is stored in memory as an in-memory table.\n",
        "* Both, Append and Complete output modes, are supported.\n",
        "* This should be used for debugging purposes on low data volumes.\n",
        "* as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.\n",
        "* Have all the aggregates in an in-memory table. The query name will be the table name, here: \"Temp_Humi_Mean\".\n",
        "* Note that we have to call start() to start the execution of the query.\n",
        "* This returns a StreamingQuery object which is a handle to the continuously running execution."
      ],
      "metadata": {
        "id": "xcns1mWVfu9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "writer = (Hourly_Mean_Value.writeStream.queryName(\"Temp_Humi_Mean\").format(\"memory\").outputMode(\"complete\").start())"
      ],
      "metadata": {
        "id": "SDSHzQWeP7Ks"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying processing details:\n",
        " * Triggering details: This indicates when to trigger the discovery and processing of newly available streaming data.\n",
        " * Checkpoint location: This option is necessary for failure recovery in the real application.\n",
        "\n",
        " For sake of simplicity, these parameters are not set by us here. By default trigger --> The streaming query executes data in micro-batches where the next micro-batch is triggered as soon as the previous micro-batch has been completed.\n",
        "\n",
        " ### Start the query\n",
        " * Once everything has been specified, the final step is to start the query, already done in the above cell by .start(). We have created a table with --> .queryName (\"Temp_Humi_Mean\") which is updated according to trigger.\n",
        "\n",
        "* This is a StreamingQuery object which is a handle to the continuously running execution. We can use this object to manage the query."
      ],
      "metadata": {
        "id": "DzRlRSXSgKbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(45):\n",
        "  print('Query Result at time step : ',x)\n",
        "  df_q = spark.sql(\"SELECT * FROM Temp_Humi_Mean\")\n",
        "  df_q.show(24)\n",
        "  time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AyaRXQwP8JX",
        "outputId": "e0ca9a34-4a2c-4303-eb70-c908ae92aa88"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Result at time step :  0\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  1\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  2\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  3\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  4\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  5\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  6\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  7\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  8\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  9\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  10\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  11\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  12\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  13\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  14\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  15\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  16\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  17\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  18\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  19\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  20\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  21\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  22\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  23\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  24\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  25\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  26\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  27\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  28\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  29\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  30\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  31\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  32\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  33\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  34\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  35\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  36\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  37\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  38\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  39\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  40\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "|  12| 20.308333333333334|68.86700266666666|\n",
            "|NULL|               NULL|             NULL|\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  41\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "|  12| 20.308333333333334|68.86700266666666|\n",
            "|NULL|               NULL|             NULL|\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  42\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "|  12| 20.308333333333334|68.86700266666666|\n",
            "|NULL|               NULL|             NULL|\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  43\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "|  22|  16.21250016666667|71.51283500000001|\n",
            "|  12| 20.308333333333334|68.86700266666666|\n",
            "|NULL|               NULL|             NULL|\n",
            "+----+-------------------+-----------------+\n",
            "\n",
            "Query Result at time step :  44\n",
            "+----+-------------------+-----------------+\n",
            "|Hour|avg(Temperatur (C))|avg(Humidity (%))|\n",
            "+----+-------------------+-----------------+\n",
            "|  22|  16.21250016666667|71.51283500000001|\n",
            "|  12| 20.308333333333334|68.86700266666666|\n",
            "|NULL|               NULL|             NULL|\n",
            "+----+-------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check if the stream is active"
      ],
      "metadata": {
        "id": "_xBszKVlgW4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.streams.active  # get the list of currently active streaming queries."
      ],
      "metadata": {
        "id": "zT4jU5oTP9e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33968da-39a3-4f31-8cd1-016539f8c607"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<pyspark.sql.streaming.query.StreamingQuery at 0x7a0edeefbb90>]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.streams.active[0].isActive"
      ],
      "metadata": {
        "id": "KEU2XHhHP-cU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68390c00-f7d0-43ce-b6bc-acb3da18acb0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer.status\n",
        "# It gives information about what the query is immediately doing - is a trigger active, is data being processed, etc.\n",
        "# Will print something like the following."
      ],
      "metadata": {
        "id": "FMWScI6_P_Ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3849e9-9024-4afa-d566-19dcc3d470b2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finally stop the query"
      ],
      "metadata": {
        "id": "hJF9Qn0zJTI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "writer.stop()  # stop the query."
      ],
      "metadata": {
        "id": "9e-a4wf7P_9Z"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}